{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f693f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "import nltk\n",
    "import json\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3588308e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/21 22:16:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://localhost:7077\") \\\n",
    "        .appName(\"AtefehAramian\")\\\n",
    "        .config(\"spark.cores.max\", 4)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark_context = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e745329",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_rdd = spark_context.textFile(\"hdfs://130.238.28.245:9000/RC_2011-08\", use_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3f3ffd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['{\"link_id\":\"t3_j4zx3\",\"score_hidden\":false,\"score\":1,\"archived\":true,\"author_flair_text\":null,\"subreddit\":\"fffffffuuuuuuuuuuuu\",\"body\":\"\\\\\"$2, would you take that deal? I\\'d take that deal\\\\\"\",\"author\":\"DorkyDude\",\"distinguished\":null,\"parent_id\":\"t3_j4zx3\",\"id\":\"c298mtc\",\"subreddit_id\":\"t5_2qqlo\",\"controversiality\":0,\"gilded\":0,\"downs\":0,\"retrieved_on\":1427415708,\"name\":\"t1_c298mtc\",\"ups\":1,\"edited\":false,\"author_flair_css_class\":null,\"created_utc\":\"1312156800\"}',\n",
       " '{\"archived\":true,\"author_flair_text\":null,\"score\":2,\"link_id\":\"t3_j4kcg\",\"score_hidden\":false,\"subreddit_id\":\"t5_2qi6d\",\"controversiality\":0,\"gilded\":0,\"parent_id\":\"t1_c293dct\",\"distinguished\":null,\"id\":\"c298mtg\",\"author\":\"TrptJim\",\"subreddit\":\"motorcycles\",\"body\":\"Have you wrecked in them yet?\",\"downs\":0,\"name\":\"t1_c298mtg\",\"retrieved_on\":1427415708,\"created_utc\":\"1312156800\",\"edited\":false,\"author_flair_css_class\":null,\"ups\":2}',\n",
       " '{\"created_utc\":\"1312156800\",\"edited\":false,\"author_flair_css_class\":\"1\",\"ups\":1,\"downs\":0,\"name\":\"t1_c298mth\",\"retrieved_on\":1427415708,\"subreddit_id\":\"t5_2rxse\",\"controversiality\":0,\"gilded\":0,\"distinguished\":null,\"parent_id\":\"t1_c298kgu\",\"id\":\"c298mth\",\"author\":\"Migeycan87\",\"subreddit\":\"reddevils\",\"body\":\"I was thinking 170k max, but if we get another player off the books (Gibson) there would be a small bit more room to maneuver?\",\"archived\":true,\"author_flair_text\":\"De Gea\",\"score\":1,\"score_hidden\":false,\"link_id\":\"t3_j53vj\"}']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_rdd.take(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cfe868",
   "metadata": {},
   "source": [
    "## Number of edited comments an its author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "470bcc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_id = comments_rdd.map(lambda line: json.loads(line)).map(lambda x: (x['author'], x['edited']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9407523",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_edited_comand_author = edited_id.filter(lambda x: x[1] == True).map(lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fc03cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pilebsa', 'brooksfosho', 'DrGhostly', 't3yrn', 'Randemonium']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_edited_comand_author.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7059978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kill spark session\n",
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19050f3a",
   "metadata": {},
   "source": [
    "### Experiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20a7964f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.4642579555511475\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "import json\n",
    "import time\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://localhost:7077\") \\\n",
    "        .appName(\"AtefehAramian\")\\\n",
    "        .config(\"spark.cores.max\", 4)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "#Start counting time\n",
    "start = time.time()\n",
    "\n",
    "#Insert analysis code here:\n",
    "comments_rdd = spark_context.textFile(\"hdfs://130.238.28.245:9000/RC_2011-08\", use_unicode=True)\n",
    "\n",
    "edited_id = comments_rdd.map(lambda line: json.loads(line)).map(lambda x: (x['author'], x['edited']))\n",
    "list_of_edited_comand_author = edited_id.filter(lambda x: x[1] == True).map(lambda x: x[0])\n",
    "list_of_edited_comand_author.take(25)\n",
    "\n",
    "#Stop counting time\n",
    "end = time.time()\n",
    "\n",
    "#Measure total time to run analysis\n",
    "result = end-start\n",
    "\n",
    "#Print result\n",
    "print(result)\n",
    "\n",
    "#Kill spark session\n",
    "spark_session.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
